{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68149773-5af3-43bd-9d2e-134e10a7252d",
   "metadata": {},
   "source": [
    "# Image to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ffc15-7cf9-4bcc-b190-10acd868af17",
   "metadata": {},
   "source": [
    "This code is to convert and save the 40,000 images for emotion calssification into numpy arrays.\n",
    "\n",
    "This is for easy access and loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856dc38a-2124-4635-a94f-e923ae9ab731",
   "metadata": {},
   "source": [
    "## Import Relevant Image Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68571c2-486a-44fb-9de0-592e3b82f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import joblib  # Import joblib for saving and loading images\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1ac9b-574a-4929-82b2-793a2a3817cf",
   "metadata": {},
   "source": [
    "## Convert images to numpy array and Store in a List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3cc45-d9e4-4420-8ded-b3e87fd902d9",
   "metadata": {},
   "source": [
    "### Splitting the image to train and test lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cae8d0-e20e-46ec-a940-dece6dc45a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#Directory containing the images\n",
    "image_dir = \"C:/Users/jcasu/Documents/CSDP 619/Facial Images\"\n",
    "\n",
    "#List to store images\n",
    "\n",
    "images_train = []\n",
    "images_test =  []\n",
    "#images_train = [[\"Images\", \"Target\"]]\n",
    "#images_test = [[\"Images\", \"Target\"]]\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for train_test_filename in os.listdir(image_dir): #options are train or test\n",
    "  for emotion_filename in os.listdir(os.path.join(image_dir, train_test_filename)): #options are angry, disgust, fear, happy, neutrakl, sad, surprise\n",
    "    for img in os.listdir(os.path.join(image_dir, train_test_filename, emotion_filename)):# This is the image\n",
    "        \n",
    "      if img.endswith('.jpg') or img.endswith('.jpeg'):  # Check for JPG/JPEG files\n",
    "          img_path = os.path.join(image_dir, train_test_filename, emotion_filename, img)\n",
    "          img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "          img = img.resize((48, 48))  # Resize to uniform size (e.g., 48x48)\n",
    "\n",
    "          if train_test_filename == 'train':\n",
    "            images_train.append([np.array(img), str(emotion_filename)])  # Convert image to array and append to the list\n",
    "          elif train_test_filename == 'test':\n",
    "            images_test.append([np.array(img), str(emotion_filename)])  # Convert image to array and append to the list\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164674fe-6bc1-4644-b637-60fdf3cc64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28709 train images\n",
      "Loaded 35887 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cached_img_test.pk1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Loaded {len(images_train)} train images')\n",
    "\n",
    "#Display the shape of the loaded images\n",
    "print(f'Loaded {len(images_train + images_test)} images')\n",
    "\n",
    "joblib.dump(images_train, 'cached_img_train.pkl')\n",
    "joblib.dump(images_test, 'cached_img_test.pk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09611cec-2d23-4b6e-8a83-7b4161e51fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28709 images\n",
      "Loaded 7178 images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Loaded {len(images_train)} images')\n",
    "print(f'Loaded {len(images_test)} images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788757bc-5829-4c88-8ee2-5475ad8e053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached images of shape (48, 48)\n",
      "Target of the image = angry\n"
     ]
    }
   ],
   "source": [
    "# Load the cached images dataset\n",
    "cached_img_train = joblib.load('cached_img_train.pkl')\n",
    "cached_img_test = joblib.load('cached_img_test.pk1')\n",
    "\n",
    "# Checking the unit data in the dataset\n",
    "print(f'Loaded cached images of shape {cached_img_train[2][0].shape}')\n",
    "print(f'Target of the image = {cached_img_train[2][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8845aaf9-4ca2-414a-bdd9-1a87e710799c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE90lEQVR4nO3bIQ7EMAwAweTU/3/ZxxaHRC2YwQZmKwPvmZkFAGut39sLAPAdogBARAGAiAIAEQUAIgoARBQAiCgAkOd0cO99cw8ALjv5VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ53RwZm7uAcAHuBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMgfGXENBwN97qoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display the first image in train images\n",
    "plt.imshow(cached_img_train[2][0], cmap='gray')\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6e563-cb8a-4a30-9c1c-eac38d0cf198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
